{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ncf7sAoOrdt-",
   "metadata": {
    "id": "Ncf7sAoOrdt-"
   },
   "source": [
    "# Movie recommendations\n",
    "\n",
    "In this notebook, we will take a stab at the task of recommending movies to potential viewers.\n",
    "\n",
    "Specifically, we will start from information of how viewers rated movies they _have_ seen, and predict how they would rate movies that they _have not_ seen, based on how other people with similar movie tastes rated those other movies. This kind of task is called _collaborative filtering_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LPiDMc9_secD",
   "metadata": {
    "id": "LPiDMc9_secD"
   },
   "source": [
    "## Prelude\n",
    "\n",
    "The only new imports are `pandas` and `csv`. Pandas is a very useful library for working with tables (called _dataframes_), in two or more dimensions. While it can sometimes be counter-intuitive, it's very powerful and well worth getting to grips with.\n",
    "\n",
    "Here, we only really use Pandas to read in the dataset CSV files, and to do some elementary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "primary-rocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.7.1\n",
      "  Using cached torch-1.7.1-cp38-none-macosx_10_9_x86_64.whl (108.9 MB)\n",
      "Collecting torchtext==0.8.1\n",
      "  Downloading torchtext-0.8.1-cp38-cp38-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision==0.8.2 in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from torch==1.7.1) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from torch==1.7.1) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from torchtext==0.8.1) (4.56.0)\n",
      "Requirement already satisfied: requests in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from torchtext==0.8.1) (2.25.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from torchvision==0.8.2) (8.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/harrison-mac/.pyenv/versions/3.8.5/envs/cpen291/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (2020.12.5)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.1\n",
      "    Uninstalling torch-1.8.1:\n",
      "      Successfully uninstalled torch-1.8.1\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "Successfully installed torch-1.7.1 torchtext-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch==1.7.1 torchtext==0.8.1 torchvision==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-driver",
   "metadata": {
    "id": "accredited-driver"
   },
   "outputs": [],
   "source": [
    "import torch, torchtext, numpy as np\n",
    "import pandas as pd, csv\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "torch.manual_seed(291)\n",
    "np.random.seed(291)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zt9V8OIypxZG",
   "metadata": {
    "id": "zt9V8OIypxZG"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8dTVSPktv52",
   "metadata": {
    "id": "u8dTVSPktv52"
   },
   "source": [
    "We're using the MovieLens 100K dataset. Actually MovieLens has much larger movie ranking datasets, and our model does even better on those, but training takes a bit more time than we have during class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "w29zg9ecKUlH",
   "metadata": {
    "id": "w29zg9ecKUlH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-09 20:30:49--  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 978202 (955K) [application/zip]\n",
      "Saving to: ‘ml-latest-small.zip’\n",
      "\n",
      "ml-latest-small.zip 100%[===================>] 955.28K  1.52MB/s    in 0.6s    \n",
      "\n",
      "2021-04-09 20:30:50 (1.52 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
      "\n",
      "Archive:  ml-latest-small.zip\n",
      "   creating: ml-latest-small/\n",
      "  inflating: ml-latest-small/links.csv  \n",
      "  inflating: ml-latest-small/tags.csv  \n",
      "  inflating: ml-latest-small/ratings.csv  \n",
      "  inflating: ml-latest-small/README.txt  \n",
      "  inflating: ml-latest-small/movies.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RWfamB_DuAS5",
   "metadata": {
    "id": "RWfamB_DuAS5"
   },
   "source": [
    "There are several CSV files. To warm up, let's look at the `movies.csv` table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "G4KqltIcpmrM",
   "metadata": {
    "id": "G4KqltIcpmrM"
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('ml-latest-small/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XM-BahskpmeK",
   "metadata": {
    "id": "XM-BahskpmeK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                      title  \\\n",
       "0           1                           Toy Story (1995)   \n",
       "1           2                             Jumanji (1995)   \n",
       "2           3                    Grumpier Old Men (1995)   \n",
       "3           4                   Waiting to Exhale (1995)   \n",
       "4           5         Father of the Bride Part II (1995)   \n",
       "...       ...                                        ...   \n",
       "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "9738   193583               No Game No Life: Zero (2017)   \n",
       "9739   193585                               Flint (2017)   \n",
       "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                           genres  \n",
       "0     Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                      Adventure|Children|Fantasy  \n",
       "2                                  Comedy|Romance  \n",
       "3                            Comedy|Drama|Romance  \n",
       "4                                          Comedy  \n",
       "...                                           ...  \n",
       "9737              Action|Animation|Comedy|Fantasy  \n",
       "9738                     Animation|Comedy|Fantasy  \n",
       "9739                                        Drama  \n",
       "9740                             Action|Animation  \n",
       "9741                                       Comedy  \n",
       "\n",
       "[9742 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SpGf2z_PuIr9",
   "metadata": {
    "id": "SpGf2z_PuIr9"
   },
   "source": [
    "Okay, so each movie has an ID (`movieId`) and other information. Because our focus is on getting recommendations based only on how other users review each movie, we will actually ignore the title, year, and genre this time.\n",
    "\n",
    "What appears to be the first, unlabelled column is actually just the row index, which Pandas keeps explicitly as part of the dataframe structure.\n",
    "\n",
    "Next, let's look at what we are _actually_ interested in, namely the movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "JzVrFvLNpmU4",
   "metadata": {
    "id": "JzVrFvLNpmU4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Qa4dBGwepmK9",
   "metadata": {
    "id": "Qa4dBGwepmK9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I6JM0cYjvD5v",
   "metadata": {
    "id": "I6JM0cYjvD5v"
   },
   "source": [
    "We have what we need: the user ID, movie ID, and the rating. First, let's look at what the ratings look like. The `df['column']` syntax just selects the column, and `.unique()` collects the unique elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41XlIUV8pmCu",
   "metadata": {
    "id": "41XlIUV8pmCu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4. , 5. , 3. , 2. , 1. , 4.5, 3.5, 2.5, 0.5, 1.5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-9N_-z3uwpI9",
   "metadata": {
    "id": "-9N_-z3uwpI9"
   },
   "source": [
    "So actually the ratings go up to 5, but they are at 0.5 intervals, so there are 10 of them.\n",
    "\n",
    "But there is something suspicious here. There are 100,835 rows, but 170,875 movies. This means some movie IDs are not mentioned here (this is called the pigeonhole principle), which means that the `movieId` dimension is sparse — but it would be most convenient for us to work with dense embedding tensors.\n",
    "\n",
    "Let's verify how many unique movies we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dZiTYVlIpl6A",
   "metadata": {
    "id": "dZiTYVlIpl6A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['movieId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUWyuGAkxWD_",
   "metadata": {
    "id": "yUWyuGAkxWD_"
   },
   "source": [
    "Not even 10,000. We will definitely have to renumber them when we create our dataset.\n",
    "\n",
    "We will also need to convert these dataframes to PyTorch tensors, so we can feed them into our model during training and testing. To do this, we need to first retrieve the actual values in the table using `.values` (this is a NumPy array), and then call `LongTensor()` on that to create a tensor of integers.\n",
    "\n",
    "The `[[ ... ]]` syntax is how one selects multiple columns in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4vB4Ue35CTvV",
   "metadata": {
    "id": "4vB4Ue35CTvV"
   },
   "outputs": [],
   "source": [
    "torch.LongTensor(df[['userId', 'movieId']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I614XyHLyHJO",
   "metadata": {
    "id": "I614XyHLyHJO"
   },
   "source": [
    "Time to build the dataset class as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BnkEd2G-q7Jc",
   "metadata": {
    "id": "BnkEd2G-q7Jc"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v6WWByIAyY40",
   "metadata": {
    "id": "v6WWByIAyY40"
   },
   "source": [
    "This is actually simpler and more straightforward than some datasets we've built before.\n",
    "\n",
    "The only new thing is that we renumber movie and user IDs so that they start from 0 and are contiguous. `u2n` and `m2n` are dictionary comprehensions — just like the list comprehensions we've seen before but these build a lookup table. Finally, `lambda` creates an unnamed function in place: for example, `lambda x: x+x` is a function that doubles its argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-statistics",
   "metadata": {
    "id": "practical-statistics"
   },
   "outputs": [],
   "source": [
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fn):\n",
    "        df = pd.read_csv(fn)\n",
    "        u2n = { u: n for n, u in enumerate(df['userId'].unique()) }\n",
    "        m2n = { m: n for n, m in enumerate(df['movieId'].unique()) }\n",
    "        df['userId'] = df['userId'].apply(lambda u: u2n[u])\n",
    "        df['movieId'] = df['movieId'].apply(lambda m: m2n[m])\n",
    "        self.coords = torch.LongTensor(df[['userId','movieId']].values)\n",
    "        self.ratings = torch.FloatTensor(df['rating'].values)\n",
    "        self.n_users = df['userId'].nunique()\n",
    "        self.n_movies = df['movieId'].nunique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.coords[i], self.ratings[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sYhFI6E5zIjt",
   "metadata": {
    "id": "sYhFI6E5zIjt"
   },
   "source": [
    "Splitting the dataset is also exactly the same as we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-superintendent",
   "metadata": {
    "id": "offensive-superintendent"
   },
   "outputs": [],
   "source": [
    "ds_full = MovieDataset('ml-latest-small/ratings.csv')\n",
    "n_train = int(0.8 * len(ds_full))\n",
    "n_test = len(ds_full) - n_train\n",
    "rng = torch.Generator().manual_seed(291)\n",
    "ds_train, ds_test = torch.utils.data.random_split(ds_full, [n_train, n_test], rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Opmi3UoOvbqO",
   "metadata": {
    "id": "Opmi3UoOvbqO"
   },
   "source": [
    "## Recommender model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GODx7cOCzQI4",
   "metadata": {
    "id": "GODx7cOCzQI4"
   },
   "source": [
    "Now that we have the dataset, we can build our model. Recall that our plan is to create embeddings from both users and movies into the same embedding space, and estimate how much the two embeddings differ by taking the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-acquisition",
   "metadata": {
    "id": "historical-acquisition"
   },
   "outputs": [],
   "source": [
    "class MovieRecs(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_dim):\n",
    "        super(MovieRecs, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_emb.weight)\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        users = self.user_emb(samples[:,0])\n",
    "        movies = self.movie_emb(samples[:,1])\n",
    "        return (users * movies).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JYa-ZIjAzqSh",
   "metadata": {
    "id": "JYa-ZIjAzqSh"
   },
   "source": [
    "Almost nothing new in the train and test code. The only difference is that `sched.step()` is called _inside_ the iteration loop, rather than outside — this is because we are using the single-cycle learning rate scheduler, which makes smooth adjustments after every minibatch rather than after every epoch.\n",
    "\n",
    "And the dataset is small enough that we don't even need a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-oregon",
   "metadata": {
    "id": "political-oregon"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "def run_test(model, ldr, crit):\n",
    "    total_loss, total_count = 0, 0\n",
    "    model.eval()\n",
    "    tq_iters = tqdm(ldr, leave=False, desc='test iter')\n",
    "    with torch.no_grad():\n",
    "        for coords, labels in tq_iters:\n",
    "            coords, labels = coords.to(device), labels.to(device)\n",
    "            preds = model(coords)\n",
    "            loss = crit(preds, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_count += labels.size(0)\n",
    "            tq_iters.set_postfix({'loss': total_loss/total_count}, refresh=True)\n",
    "    return total_loss / total_count\n",
    "\n",
    "def run_train(model, ldr, crit, opt, sched):\n",
    "    model.train()\n",
    "    total_loss, total_count = 0, 0\n",
    "    tq_iters = tqdm(ldr, leave=False, desc='train iter')\n",
    "    for (coords, labels) in tq_iters:\n",
    "        opt.zero_grad()\n",
    "        coords, labels = coords.to(device), labels.to(device)\n",
    "        preds = model(coords)\n",
    "        loss = crit(preds, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_count += labels.size(0)\n",
    "        tq_iters.set_postfix({'loss': total_loss/total_count}, refresh=True)\n",
    "    return total_loss / total_count\n",
    "\n",
    "def run_all(model, ldr_train, ldr_test, crit, opt, sched, n_epochs=10):\n",
    "    best_loss = np.inf\n",
    "    tq_epochs = tqdm(range(n_epochs), desc='epochs', unit='ep')\n",
    "    for epoch in tq_epochs:\n",
    "        train_loss = run_train(model, ldr_train, crit, opt, sched)\n",
    "        test_loss = run_test(model, ldr_test, crit)\n",
    "        tqdm.write(f'epoch {epoch}   train loss {train_loss:.6f}    test loss {test_loss:.6f}')\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            tq_epochs.set_postfix({'bE': epoch, 'bL': best_loss}, refresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uVO1RF9u0BXR",
   "metadata": {
    "id": "uVO1RF9u0BXR"
   },
   "source": [
    "Again, only two things new here.\n",
    "\n",
    "First, we are using mean squared error (MSE) as the loss function. This is only because results on this dataset are normally reported using RMSE (R = root), which PyTorch does not have. We could, of course, easily write an RMSE loss — but minimizing MSE will also minimize the RMSE (because sqrt is strictly increasing), so MSE works just as well.\n",
    "\n",
    "Second, we are using `OneCycleLR()`, mostly because I happened to try that first and it worked well. Often this gives very good results more quickly than other LR schedules, so it's almost always worth trying. This means that we will need to make a `sched.step()` adjustment as described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-repository",
   "metadata": {
    "id": "digital-repository"
   },
   "outputs": [],
   "source": [
    "model = MovieRecs(ds_full.n_users, ds_full.n_movies, 20)\n",
    "model.to(device)\n",
    "\n",
    "ldr_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "ldr_test = torch.utils.data.DataLoader(ds_test, batch_size=32)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "crit = nn.MSELoss().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n",
    "sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.1, steps_per_epoch=len(ldr_train), epochs=n_epochs)\n",
    "\n",
    "run_all(model, ldr_train, ldr_test, crit, opt, sched, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W6tnFqmg2Fz7",
   "metadata": {
    "id": "W6tnFqmg2Fz7"
   },
   "source": [
    "This is already pretty good — try for yourself to evaluate a random, untrained model and compare — but it turns out that we can do even better by making the model slightly more complicated.\n",
    "\n",
    "As we wrote it, the model computes ratings from (user, movie) pairs. But this makes it difficult to account for people who always write bad (or good) reviews, and for movies that are universally considered terrible or amazing. (Or even so terrible that one finds oneself transfixed.)\n",
    "\n",
    "Actually, the model _could_ learn about grumpy reviewers, but it would have to learn this _separately_ for every movie. We can make learning this much easier by adding bias. Let's try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meGQh2HivkiK",
   "metadata": {
    "id": "meGQh2HivkiK"
   },
   "source": [
    "## Recommender model with bias\n",
    "\n",
    "Let's think what bias means in our case. We want it to learn one value (i.e., the bias offset) separately for every reviewer, and another set of value for the movies. This means that our bias is also an embedding: we index it using the movie (or user) ID, and we get back a single number.\n",
    "\n",
    "The only fly in the ointment is that this gives us a rank-one tensor with one dimensions, but we can `squeeze()` that extra encapsulation away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-emerald",
   "metadata": {
    "id": "nuclear-emerald"
   },
   "outputs": [],
   "source": [
    "class MovieRecs(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_dim):\n",
    "        super(MovieRecs, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
    "        self.movie_bias = nn.Embedding(n_movies, 1)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.movie_emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.movie_bias.weight)\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        users = self.user_emb(samples[:,0])\n",
    "        movies = self.movie_emb(samples[:,1])\n",
    "        dot = (users * movies).sum(1)\n",
    "        user_b = self.user_bias(samples[:,0]).squeeze()\n",
    "        movie_b = self.movie_bias(samples[:,1]).squeeze()\n",
    "        return dot + user_b + movie_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-chambers",
   "metadata": {
    "id": "wireless-chambers"
   },
   "outputs": [],
   "source": [
    "model = MovieRecs(ds_full.n_users, ds_full.n_movies, 20)\n",
    "model.to(device)\n",
    "\n",
    "ldr_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "ldr_test = torch.utils.data.DataLoader(ds_test, batch_size=32)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "crit = nn.MSELoss().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n",
    "sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.1, steps_per_epoch=len(ldr_train), epochs=n_epochs)\n",
    "\n",
    "run_all(model, ldr_train, ldr_test, crit, opt, sched, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JdC0ovap5bTh",
   "metadata": {
    "id": "JdC0ovap5bTh"
   },
   "source": [
    "This converged much faster for me than the previous version and gave me better results.\n",
    "\n",
    "There are a few other simple things I tried, like clamping the predicted rating range in various ways (e.g., sigmoid), regularization, messing with the embedding dimension, and so on. Some of them help a little bit — but they're easy enough to try for yourself.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "recommender-system.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
